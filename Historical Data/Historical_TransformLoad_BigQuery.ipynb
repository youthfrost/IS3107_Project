{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL pipeline for Loading Historical Youtube Trending Data into BigQuery\n",
    "\n",
    "1. **Extract:** Extract Parquet Data from Google Cloud Storage\n",
    "2. **Transform:** Do Transformations on Data \n",
    "3. **Load:** Load Transformed Data in BigQuery Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Using cached google_cloud_bigquery-3.31.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.39.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.7.2)\n",
      "Requirement already satisfied: packaging>=24.2.0 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-bigquery) (24.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.67.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading grpcio_status-1.72.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.1.31)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading protobuf-6.31.0rc1-cp39-abi3-macosx_10_9_universal2.whl.metadata (596 bytes)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading grpcio-1.72.0rc1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/cedricyee/Documents/IS3107/.conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n",
      "Using cached google_cloud_bigquery-3.31.0-py3-none-any.whl (250 kB)\n",
      "Downloading grpcio_status-1.72.0rc1-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.72.0rc1-cp311-cp311-macosx_11_0_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.0rc1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Installing collected packages: protobuf, grpcio, grpcio-status, google-cloud-bigquery\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.67.1\n",
      "    Uninstalling grpcio-1.67.1:\n",
      "      Successfully uninstalled grpcio-1.67.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.0rc1 which is incompatible.\n",
      "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\n",
      "streamlit 1.44.1 requires protobuf<6,>=3.20, but you have protobuf 6.31.0rc1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-bigquery-3.31.0 grpcio-1.71.0 grpcio-status-1.72.0rc1 protobuf-6.31.0rc1\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import statistics as st\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"aesthetic-nova-454803-r7-94e7eb0af61c.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Parquet File Stored in Google Cloud Storage (GCS) bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3C66w5Z0ixs</td>\n",
       "      <td>I ASKED HER TO BE MY GIRLFRIEND...</td>\n",
       "      <td>2020-08-11 19:20:14+00:00</td>\n",
       "      <td>UCvtRTOMP2TqYqu51xNrqAzg</td>\n",
       "      <td>Brawadis</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>2020-08-12 00:00:00+00:00</td>\n",
       "      <td>brawadis|prank|basketball|skits|ghost|funny vi...</td>\n",
       "      <td>1514614</td>\n",
       "      <td>156908</td>\n",
       "      <td>5855</td>\n",
       "      <td>35313</td>\n",
       "      <td>https://i.ytimg.com/vi/3C66w5Z0ixs/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SUBSCRIBE to BRAWADIS ▶ http://bit.ly/Subscrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M9Pmf9AB4Mo</td>\n",
       "      <td>Apex Legends | Stories from the Outlands – “Th...</td>\n",
       "      <td>2020-08-11 17:00:10+00:00</td>\n",
       "      <td>UC0ZV6M2THA81QT9hrVWJG3A</td>\n",
       "      <td>Apex Legends</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>2020-08-12 00:00:00+00:00</td>\n",
       "      <td>Apex Legends|Apex Legends characters|new Apex ...</td>\n",
       "      <td>2381688</td>\n",
       "      <td>146739</td>\n",
       "      <td>2794</td>\n",
       "      <td>16549</td>\n",
       "      <td>https://i.ytimg.com/vi/M9Pmf9AB4Mo/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>While running her own modding shop, Ramya Pare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J78aPJ3VyNs</td>\n",
       "      <td>I left youtube for a month and THIS is what ha...</td>\n",
       "      <td>2020-08-11 16:34:06+00:00</td>\n",
       "      <td>UCYzPXprvl5Y-Sf0g4vX-m6g</td>\n",
       "      <td>jacksepticeye</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2020-08-12 00:00:00+00:00</td>\n",
       "      <td>jacksepticeye|funny|funny meme|memes|jacksepti...</td>\n",
       "      <td>2038853</td>\n",
       "      <td>353787</td>\n",
       "      <td>2628</td>\n",
       "      <td>40221</td>\n",
       "      <td>https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I left youtube for a month and this is what ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kXLn3HkpjaA</td>\n",
       "      <td>XXL 2020 Freshman Class Revealed - Official An...</td>\n",
       "      <td>2020-08-11 16:38:55+00:00</td>\n",
       "      <td>UCbg_UMjlHJg_19SZckaKajg</td>\n",
       "      <td>XXL</td>\n",
       "      <td>Music</td>\n",
       "      <td>2020-08-12 00:00:00+00:00</td>\n",
       "      <td>xxl freshman|xxl freshmen|2020 xxl freshman|20...</td>\n",
       "      <td>496771</td>\n",
       "      <td>23251</td>\n",
       "      <td>1856</td>\n",
       "      <td>7647</td>\n",
       "      <td>https://i.ytimg.com/vi/kXLn3HkpjaA/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Subscribe to XXL → http://bit.ly/subscribe-xxl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIUo6yapDbc</td>\n",
       "      <td>Ultimate DIY Home Movie Theater for The LaBran...</td>\n",
       "      <td>2020-08-11 15:10:05+00:00</td>\n",
       "      <td>UCDVPcEbVLQgLZX0Rt6jo34A</td>\n",
       "      <td>Mr. Kate</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "      <td>2020-08-12 00:00:00+00:00</td>\n",
       "      <td>The LaBrant Family|DIY|Interior Design|Makeove...</td>\n",
       "      <td>1123889</td>\n",
       "      <td>45802</td>\n",
       "      <td>964</td>\n",
       "      <td>2196</td>\n",
       "      <td>https://i.ytimg.com/vi/VIUo6yapDbc/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Transforming The LaBrant Family's empty white ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  3C66w5Z0ixs                 I ASKED HER TO BE MY GIRLFRIEND...   \n",
       "1  M9Pmf9AB4Mo  Apex Legends | Stories from the Outlands – “Th...   \n",
       "2  J78aPJ3VyNs  I left youtube for a month and THIS is what ha...   \n",
       "3  kXLn3HkpjaA  XXL 2020 Freshman Class Revealed - Official An...   \n",
       "4  VIUo6yapDbc  Ultimate DIY Home Movie Theater for The LaBran...   \n",
       "\n",
       "                publishedAt                 channelId   channelTitle  \\\n",
       "0 2020-08-11 19:20:14+00:00  UCvtRTOMP2TqYqu51xNrqAzg       Brawadis   \n",
       "1 2020-08-11 17:00:10+00:00  UC0ZV6M2THA81QT9hrVWJG3A   Apex Legends   \n",
       "2 2020-08-11 16:34:06+00:00  UCYzPXprvl5Y-Sf0g4vX-m6g  jacksepticeye   \n",
       "3 2020-08-11 16:38:55+00:00  UCbg_UMjlHJg_19SZckaKajg            XXL   \n",
       "4 2020-08-11 15:10:05+00:00  UCDVPcEbVLQgLZX0Rt6jo34A       Mr. Kate   \n",
       "\n",
       "       categoryId             trending_date  \\\n",
       "0  People & Blogs 2020-08-12 00:00:00+00:00   \n",
       "1          Gaming 2020-08-12 00:00:00+00:00   \n",
       "2   Entertainment 2020-08-12 00:00:00+00:00   \n",
       "3           Music 2020-08-12 00:00:00+00:00   \n",
       "4   Howto & Style 2020-08-12 00:00:00+00:00   \n",
       "\n",
       "                                                tags  view_count   likes  \\\n",
       "0  brawadis|prank|basketball|skits|ghost|funny vi...     1514614  156908   \n",
       "1  Apex Legends|Apex Legends characters|new Apex ...     2381688  146739   \n",
       "2  jacksepticeye|funny|funny meme|memes|jacksepti...     2038853  353787   \n",
       "3  xxl freshman|xxl freshmen|2020 xxl freshman|20...      496771   23251   \n",
       "4  The LaBrant Family|DIY|Interior Design|Makeove...     1123889   45802   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0      5855          35313  https://i.ytimg.com/vi/3C66w5Z0ixs/default.jpg   \n",
       "1      2794          16549  https://i.ytimg.com/vi/M9Pmf9AB4Mo/default.jpg   \n",
       "2      2628          40221  https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg   \n",
       "3      1856           7647  https://i.ytimg.com/vi/kXLn3HkpjaA/default.jpg   \n",
       "4       964           2196  https://i.ytimg.com/vi/VIUo6yapDbc/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  \\\n",
       "0              False             False   \n",
       "1              False             False   \n",
       "2              False             False   \n",
       "3              False             False   \n",
       "4              False             False   \n",
       "\n",
       "                                         description  \n",
       "0  SUBSCRIBE to BRAWADIS ▶ http://bit.ly/Subscrib...  \n",
       "1  While running her own modding shop, Ramya Pare...  \n",
       "2  I left youtube for a month and this is what ha...  \n",
       "3  Subscribe to XXL → http://bit.ly/subscribe-xxl...  \n",
       "4  Transforming The LaBrant Family's empty white ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = \"youtube-trending-videos-dataset\"\n",
    "PARQUET_BLOB_PATH = \"youtube_trending_data/US_youtube_trending_data.parquet\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def download_blob_to_temp(bucket_name, source_blob_name, suffix=\"\"):\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    blob.download_to_filename(temp_file.name)\n",
    "    return temp_file.name\n",
    "\n",
    "# Download Parquet file to a temporary file\n",
    "parquet_temp_path = download_blob_to_temp(BUCKET_NAME, PARQUET_BLOB_PATH, suffix=\".parquet\")\n",
    "\n",
    "df = pd.read_parquet(parquet_temp_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268787, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47142"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['video_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are 268787 `video_id` found, but only 47142 of them are unique. Hence, we conduct a check for duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58421</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-05 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>1614230</td>\n",
       "      <td>106457</td>\n",
       "      <td>1754</td>\n",
       "      <td>5555</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58624</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-05 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2021264</td>\n",
       "      <td>124460</td>\n",
       "      <td>2019</td>\n",
       "      <td>6052</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58843</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-06 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2198527</td>\n",
       "      <td>132458</td>\n",
       "      <td>2163</td>\n",
       "      <td>6211</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59052</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-06 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2327535</td>\n",
       "      <td>137953</td>\n",
       "      <td>2246</td>\n",
       "      <td>6333</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59266</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-07 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2446440</td>\n",
       "      <td>142586</td>\n",
       "      <td>2340</td>\n",
       "      <td>6412</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59478</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-07 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2529782</td>\n",
       "      <td>145383</td>\n",
       "      <td>2375</td>\n",
       "      <td>6497</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59706</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-08 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2611407</td>\n",
       "      <td>148382</td>\n",
       "      <td>2419</td>\n",
       "      <td>6580</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59907</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-08 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2679077</td>\n",
       "      <td>150412</td>\n",
       "      <td>2457</td>\n",
       "      <td>6629</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60136</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-09 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2744775</td>\n",
       "      <td>152488</td>\n",
       "      <td>2500</td>\n",
       "      <td>6697</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60338</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-09 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2791302</td>\n",
       "      <td>153886</td>\n",
       "      <td>2514</td>\n",
       "      <td>6718</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60576</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-10 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2828987</td>\n",
       "      <td>154949</td>\n",
       "      <td>2530</td>\n",
       "      <td>6742</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60776</th>\n",
       "      <td>xaKAn0zHMH0</td>\n",
       "      <td>World's Hottest Gummy Bear (9,000,000 Scoville)</td>\n",
       "      <td>2021-06-03 20:03:16+00:00</td>\n",
       "      <td>UCd1fLoVFooPeWqCEYVUJZqg</td>\n",
       "      <td>Matt Stonie</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2021-06-10 00:00:00+00:00</td>\n",
       "      <td>Matt Stonie|Megatoad|Competitive Eating|Food C...</td>\n",
       "      <td>2853659</td>\n",
       "      <td>155550</td>\n",
       "      <td>2544</td>\n",
       "      <td>6760</td>\n",
       "      <td>https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A portion of this video is sponsored by The Mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                            title  \\\n",
       "58421  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "58624  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "58843  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "59052  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "59266  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "59478  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "59706  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "59907  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "60136  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "60338  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "60576  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "60776  xaKAn0zHMH0  World's Hottest Gummy Bear (9,000,000 Scoville)   \n",
       "\n",
       "                    publishedAt                 channelId channelTitle  \\\n",
       "58421 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "58624 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "58843 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "59052 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "59266 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "59478 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "59706 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "59907 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "60136 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "60338 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "60576 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "60776 2021-06-03 20:03:16+00:00  UCd1fLoVFooPeWqCEYVUJZqg  Matt Stonie   \n",
       "\n",
       "          categoryId             trending_date  \\\n",
       "58421  Entertainment 2021-06-05 00:00:00+00:00   \n",
       "58624  Entertainment 2021-06-05 00:00:00+00:00   \n",
       "58843  Entertainment 2021-06-06 00:00:00+00:00   \n",
       "59052  Entertainment 2021-06-06 00:00:00+00:00   \n",
       "59266  Entertainment 2021-06-07 00:00:00+00:00   \n",
       "59478  Entertainment 2021-06-07 00:00:00+00:00   \n",
       "59706  Entertainment 2021-06-08 00:00:00+00:00   \n",
       "59907  Entertainment 2021-06-08 00:00:00+00:00   \n",
       "60136  Entertainment 2021-06-09 00:00:00+00:00   \n",
       "60338  Entertainment 2021-06-09 00:00:00+00:00   \n",
       "60576  Entertainment 2021-06-10 00:00:00+00:00   \n",
       "60776  Entertainment 2021-06-10 00:00:00+00:00   \n",
       "\n",
       "                                                    tags  view_count   likes  \\\n",
       "58421  Matt Stonie|Megatoad|Competitive Eating|Food C...     1614230  106457   \n",
       "58624  Matt Stonie|Megatoad|Competitive Eating|Food C...     2021264  124460   \n",
       "58843  Matt Stonie|Megatoad|Competitive Eating|Food C...     2198527  132458   \n",
       "59052  Matt Stonie|Megatoad|Competitive Eating|Food C...     2327535  137953   \n",
       "59266  Matt Stonie|Megatoad|Competitive Eating|Food C...     2446440  142586   \n",
       "59478  Matt Stonie|Megatoad|Competitive Eating|Food C...     2529782  145383   \n",
       "59706  Matt Stonie|Megatoad|Competitive Eating|Food C...     2611407  148382   \n",
       "59907  Matt Stonie|Megatoad|Competitive Eating|Food C...     2679077  150412   \n",
       "60136  Matt Stonie|Megatoad|Competitive Eating|Food C...     2744775  152488   \n",
       "60338  Matt Stonie|Megatoad|Competitive Eating|Food C...     2791302  153886   \n",
       "60576  Matt Stonie|Megatoad|Competitive Eating|Food C...     2828987  154949   \n",
       "60776  Matt Stonie|Megatoad|Competitive Eating|Food C...     2853659  155550   \n",
       "\n",
       "       dislikes  comment_count  \\\n",
       "58421      1754           5555   \n",
       "58624      2019           6052   \n",
       "58843      2163           6211   \n",
       "59052      2246           6333   \n",
       "59266      2340           6412   \n",
       "59478      2375           6497   \n",
       "59706      2419           6580   \n",
       "59907      2457           6629   \n",
       "60136      2500           6697   \n",
       "60338      2514           6718   \n",
       "60576      2530           6742   \n",
       "60776      2544           6760   \n",
       "\n",
       "                                       thumbnail_link  comments_disabled  \\\n",
       "58421  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "58624  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "58843  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "59052  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "59266  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "59478  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "59706  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "59907  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "60136  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "60338  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "60576  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "60776  https://i.ytimg.com/vi/xaKAn0zHMH0/default.jpg              False   \n",
       "\n",
       "       ratings_disabled                                        description  \n",
       "58421             False  A portion of this video is sponsored by The Mo...  \n",
       "58624             False  A portion of this video is sponsored by The Mo...  \n",
       "58843             False  A portion of this video is sponsored by The Mo...  \n",
       "59052             False  A portion of this video is sponsored by The Mo...  \n",
       "59266             False  A portion of this video is sponsored by The Mo...  \n",
       "59478             False  A portion of this video is sponsored by The Mo...  \n",
       "59706             False  A portion of this video is sponsored by The Mo...  \n",
       "59907             False  A portion of this video is sponsored by The Mo...  \n",
       "60136             False  A portion of this video is sponsored by The Mo...  \n",
       "60338             False  A portion of this video is sponsored by The Mo...  \n",
       "60576             False  A portion of this video is sponsored by The Mo...  \n",
       "60776             False  A portion of this video is sponsored by The Mo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter `video_id` that appear more than once; Example 10\n",
    "video_ids_with_10plus = df['video_id'].value_counts()\n",
    "video_ids_with_10plus = video_ids_with_10plus[video_ids_with_10plus > 10].index\n",
    "\n",
    "# Randomly pick one of those `video_id`\n",
    "example_id = np.random.choice(video_ids_with_10plus)\n",
    "\n",
    "df[df['video_id'] == example_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, we can observe that for the same video retrieved, as the `trending_date` becomes larger, video engagement metrics such as `view_counts`, `likes`, `dislikes`, `comment_count` also increases. This represents the exact duration for which the same video was found in the trending videos list, which we would like to explore more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Pipeline from GCS -> BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we extract the GCS bucket data containing our historical data, we conduct 1 round of **Transformation** by aggregating video features by their entire trending period to avoid video duplication as seen above (ie. 1 trending video for 10 days; 10 records -> 1 record). We also perform feature engineering to obtain derived features (from original features) as well as new features we would like to analyse (`popularity_score`, `popularity_class`).\n",
    "\n",
    "After Transformation, we **Load** the data into a BigQuery table `US_trending_videos_transformed` where we are then able to query the data for our downstream Machine Learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 00:47:56,073 - INFO - Starting ETL pipeline\n",
      "2025-04-09 00:47:56,073 - INFO - Extracting data from gs://youtube-trending-videos-dataset/youtube_trending_data/US_youtube_trending_data.parquet\n",
      "2025-04-09 00:48:07,061 - INFO - Extracted 268787 rows of data\n",
      "2025-04-09 00:48:07,063 - INFO - Starting data transformation\n",
      "/var/folders/nh/z8kvm2x92hn0fh3j63wxtsfw0000gn/T/ipykernel_5030/1846737585.py:70: FutureWarning: The provided callable <function min at 0x10c815440> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  transformed_df = df1.groupby('video_id').agg({'title': st.mode, 'publishedAt': np.min, 'channelId': st.mode, 'channelTitle': st.mode, 'categoryId': st.mode, 'tags': st.mode,\n",
      "2025-04-09 00:48:11,485 - INFO - Transformation complete. DataFrame now has 25 columns\n",
      "2025-04-09 00:48:11,488 - INFO - DataFrame preview (first 5 rows):\n",
      "2025-04-09 00:48:11,511 - INFO - Loading transformed data to gs://youtube-trending-videos-dataset/youtube_trending_data/transformed/US_youtube_trending_data_transformed.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Preview:\n",
      "Shape: (47142, 25)\n",
      "\n",
      "Sample Data:\n",
      "      video_id                                              title  \\\n",
      "0  --14w5SOEUs                 Migos - Avalanche (Official Video)   \n",
      "1  --2O86Z0hsM                           MY TESLA PAYS FOR ITSELF   \n",
      "2  --40TEbZ9Is         Supporting Actress in a Comedy: 73rd Emmys   \n",
      "3  --47FjCWgrU  San Francisco 49ers vs. Arizona Cardinals Game...   \n",
      "4  --5-brQiQFg  Washington Commanders vs. San Francisco 49ers ...   \n",
      "\n",
      "                  channelId        channelTitle     categoryId  \\\n",
      "0  UCGIelM2Dj3zza3xyV3pL3WQ           MigosVEVO          Music   \n",
      "1  UCXJEvxZSozjAAqhbMfhIArA             jf.okay  Entertainment   \n",
      "2  UClBKH8yZRcM4AsRjDVEdjMg  Television Academy  Entertainment   \n",
      "3  UCDVYQ4Zhbm3S2dlz7P1GBDg                 NFL         Sports   \n",
      "4  UCDVYQ4Zhbm3S2dlz7P1GBDg                 NFL         Sports   \n",
      "\n",
      "                                                tags  comments_disabled  \\\n",
      "0  Migos|Avalanche|Quality|Control|Music/Motown|R...              False   \n",
      "1                                             [None]              False   \n",
      "2                                             [None]              False   \n",
      "3                                             [None]              False   \n",
      "4                                             [None]              False   \n",
      "\n",
      "   ratings_disabled                                        description  \\\n",
      "0             False  Music video by Migos performing Avalanche. Qua...   \n",
      "1             False                                            Unknown   \n",
      "2             False  Hannah Waddingham wins the Emmy for Supporting...   \n",
      "3             False  Check out our other channels:NFL Mundo https:/...   \n",
      "4             False  Check out our other channels:NFL Mundo https:/...   \n",
      "\n",
      "   likes_start  ...  trendingDuration  hoursToReachTrending  \\\n",
      "0       122830  ...                 4                   8.0   \n",
      "1        16481  ...                 4                  24.7   \n",
      "2         6299  ...                 4                  22.9   \n",
      "3        20234  ...                 6                  23.6   \n",
      "4        14603  ...                 0                  23.5   \n",
      "\n",
      "   published_dayOfWeek  published_year  published_month  tagCount  \\\n",
      "0             Thursday            2021                6         8   \n",
      "1            Wednesday            2022                3         1   \n",
      "2               Monday            2021                9         1   \n",
      "3               Monday            2023               12         1   \n",
      "4               Sunday            2022               12         1   \n",
      "\n",
      "   engagement_rate like_view_ratio  popularity_score  popularity_class  \n",
      "0         0.002410        0.038500        633.125061                 3  \n",
      "1         0.002672        0.032109         46.204277                 2  \n",
      "2         0.001059        0.011762          8.504088                 1  \n",
      "3         0.001040        0.011651         23.511677                 1  \n",
      "4         0.001622        0.011400         23.688607                 1  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "Column Data Types:\n",
      "video_id                 object\n",
      "title                    object\n",
      "channelId                object\n",
      "channelTitle             object\n",
      "categoryId               object\n",
      "tags                     object\n",
      "comments_disabled          bool\n",
      "ratings_disabled           bool\n",
      "description              object\n",
      "likes_start               int64\n",
      "likes_end                 int64\n",
      "view_count_start          int64\n",
      "view_count_end            int64\n",
      "comment_count_start       int64\n",
      "comment_count_end         int64\n",
      "trendingDuration          int64\n",
      "hoursToReachTrending    float64\n",
      "published_dayOfWeek      object\n",
      "published_year            int32\n",
      "published_month           int32\n",
      "tagCount                  int64\n",
      "engagement_rate         float64\n",
      "like_view_ratio         float64\n",
      "popularity_score        float64\n",
      "popularity_class          int64\n",
      "dtype: object\n",
      "\n",
      "Summary Statistics:\n",
      "       likes_start    likes_end  view_count_start  view_count_end  \\\n",
      "count     47142.00     47142.00          47142.00    4.714200e+04   \n",
      "mean      77302.80    119479.71        1244531.30    2.669534e+06   \n",
      "std      220383.21    365096.18        2881945.05    9.624392e+06   \n",
      "min           0.00         0.00              0.00    0.000000e+00   \n",
      "25%       13129.25     18340.00         313158.50    5.388602e+05   \n",
      "50%       27912.50     41724.50         570583.00    1.066956e+06   \n",
      "75%       65363.75    101793.75        1194770.75    2.324437e+06   \n",
      "max     7114337.00  16021534.00       91463891.00    1.407644e+09   \n",
      "\n",
      "       comment_count_start  comment_count_end  trendingDuration  \\\n",
      "count             47142.00           47142.00          47142.00   \n",
      "mean               6354.21            8994.08              4.77   \n",
      "std               37333.66           67410.20              1.99   \n",
      "min                   0.00               0.00              0.00   \n",
      "25%                 978.00            1300.25              4.00   \n",
      "50%                2077.00            2759.50              5.00   \n",
      "75%                4642.00            6206.00              6.00   \n",
      "max             3400291.00         6738537.00             36.00   \n",
      "\n",
      "       hoursToReachTrending  published_year  published_month  tagCount  \\\n",
      "count              47142.00        47142.00         47142.00  47142.00   \n",
      "mean                  21.46         2021.88             6.65     16.17   \n",
      "std                   19.36            1.10             3.53     12.55   \n",
      "min                  -14.00         2020.00             1.00      1.00   \n",
      "25%                    9.00         2021.00             3.00      5.00   \n",
      "50%                   21.60         2022.00             7.00     15.00   \n",
      "75%                   28.20         2023.00            10.00     25.00   \n",
      "max                  699.70         2024.00            12.00     78.00   \n",
      "\n",
      "       engagement_rate  like_view_ratio  popularity_score  popularity_class  \n",
      "count         47129.00         47131.00          47127.00          47142.00  \n",
      "mean              0.00             0.05            595.29              2.00  \n",
      "std               0.00             0.03           4948.98              0.71  \n",
      "min               0.00             0.00              0.00              0.00  \n",
      "25%               0.00             0.02             36.47              1.00  \n",
      "50%               0.00             0.04            108.66              2.00  \n",
      "75%               0.00             0.06            312.64              2.00  \n",
      "max               0.16             0.33         410229.76              3.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 00:48:16,144 - INFO - Successfully saved transformed data to gs://youtube-trending-videos-dataset/youtube_trending_data/transformed/US_youtube_trending_data_transformed.parquet\n",
      "2025-04-09 00:48:16,147 - INFO - Loading data from gs://youtube-trending-videos-dataset/youtube_trending_data/transformed/US_youtube_trending_data_transformed.parquet to BigQuery table US_trending_videos_transformed\n",
      "2025-04-09 00:48:23,505 - INFO - Loaded 47142 rows and 25 columns to aesthetic-nova-454803-r7.youtube_trending_dataset.US_trending_videos_transformed\n",
      "2025-04-09 00:48:23,506 - INFO - ETL pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set up basic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class YouTubeTrendingETL:\n",
    "    def __init__(self, gcp_project_id, dataset_id, bucket_name):\n",
    "        \"\"\"\n",
    "        Initialize the ETL pipeline with GCP project, dataset ID, and bucket name.\n",
    "        \n",
    "        :param gcp_project_id: GCP Project ID\n",
    "        :param dataset_id: BigQuery dataset ID\n",
    "        :param bucket_name: Google Cloud Storage bucket name\n",
    "        \"\"\"\n",
    "        self.gcp_project_id = gcp_project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.bucket_name = bucket_name\n",
    "        self.bq_client = bigquery.Client(project=gcp_project_id)\n",
    "        self.storage_client = storage.Client(project=gcp_project_id)\n",
    "        self.bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "    def extract_from_gcs(self, parquet_blob_path):\n",
    "        \"\"\"\n",
    "        Extract data from a parquet file in GCS\n",
    "        \n",
    "        :param parquet_blob_path: Path to the parquet file in the GCS bucket\n",
    "        :return: Pandas DataFrame containing the data\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting data from gs://{self.bucket_name}/{parquet_blob_path}\")\n",
    "        \n",
    "        # Create a temporary file to download the parquet\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.parquet')\n",
    "        blob = self.bucket.blob(parquet_blob_path)\n",
    "        blob.download_to_filename(temp_file.name)\n",
    "        \n",
    "        df = pd.read_parquet(temp_file.name)\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        temp_file.close()\n",
    "        os.unlink(temp_file.name)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(df)} rows of data\")\n",
    "        return df\n",
    "\n",
    "    def transform_data(self, df):\n",
    "        \"\"\"\n",
    "        Transform the YouTube trending videos data\n",
    "        \n",
    "        :param df: Pandas DataFrame containing the raw data\n",
    "        :return: Transformed Pandas DataFrame\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data transformation\")\n",
    "        \n",
    "        # Make a copy to avoid modifying the original dataframe\n",
    "        df1 = df.copy()\n",
    "\n",
    "        # Dropping `dislikes` column (discontinued by YouTube from 2020-2021)\n",
    "        df1.drop(columns=['dislikes'], inplace=True, errors='ignore')\n",
    "\n",
    "        def start_end_trending(x: list) -> list:\n",
    "            return [min(x), max(x)]\n",
    "\n",
    "        def description_transf(x: list) -> str:\n",
    "            return pd.Series(x).fillna('Unknown').iloc[0]\n",
    "\n",
    "        # Grouping by 'video_id' and aggregating using functions as specified in the above cell\n",
    "        transformed_df = df1.groupby('video_id').agg({'title': st.mode, 'publishedAt': np.min, 'channelId': st.mode, 'channelTitle': st.mode, 'categoryId': st.mode, 'tags': st.mode,\n",
    "                    'trending_date': start_end_trending, 'likes': start_end_trending, 'view_count': start_end_trending, 'comment_count': start_end_trending, 'comments_disabled': st.mode, \n",
    "                    'ratings_disabled': st.mode, 'description':description_transf}).reset_index()\n",
    "\n",
    "        # Extract from (min, max)\n",
    "        transformed_df['trending_date_start'] = transformed_df['trending_date'].apply(lambda x: min(x))\n",
    "        transformed_df['trending_date_end'] = transformed_df['trending_date'].apply(lambda x: max(x))\n",
    "        transformed_df['likes_start'] = transformed_df['likes'].apply(lambda x: min(x))\n",
    "        transformed_df['likes_end'] = transformed_df['likes'].apply(lambda x: max(x))\n",
    "        transformed_df['view_count_start'] = transformed_df['view_count'].apply(lambda x: min(x))\n",
    "        transformed_df['view_count_end'] = transformed_df['view_count'].apply(lambda x: max(x))\n",
    "        transformed_df['comment_count_start'] = transformed_df['comment_count'].apply(lambda x: min(x))\n",
    "        transformed_df['comment_count_end'] = transformed_df['comment_count'].apply(lambda x: max(x))   \n",
    "\n",
    "        # Number of days in trending\n",
    "        transformed_df['trendingDuration'] = (transformed_df['trending_date_end'] - transformed_df['trending_date_start']).dt.days\n",
    "\n",
    "        # Time Taken to Trend (in Hours)\n",
    "        transformed_df['hoursToReachTrending'] = round((transformed_df['trending_date_start'] - transformed_df['publishedAt']).dt.total_seconds() / (60 * 60), 1)\n",
    "\n",
    "        # Extracting the day of the week\n",
    "        transformed_df['published_dayOfWeek'] = transformed_df['publishedAt'].dt.day_name()\n",
    "\n",
    "        # Extracting published Year of each video\n",
    "        transformed_df['published_year'] = transformed_df['publishedAt'].dt.year\n",
    "\n",
    "        # Extracting published month of each video\n",
    "        transformed_df['published_month'] = transformed_df['publishedAt'].dt.month   \n",
    "\n",
    "        # Number of tags present\n",
    "        transformed_df['tagCount'] = transformed_df['tags'].apply(lambda x: 0 if type(x) == float else len(list(x.split('|'))))\n",
    "\n",
    "        \n",
    "        transformed_df.drop(['trending_date', 'likes', 'publishedAt', 'view_count', 'trending_date_start', 'trending_date_end', 'comment_count'], axis = 1, inplace = True)\n",
    "\n",
    "        # Aggregating the engagement rate: We will take the average of the comment_count and view_count over the trending period\n",
    "        transformed_df['engagement_rate'] = (transformed_df['comment_count_end'] / \n",
    "                                            transformed_df['view_count_end']).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        # Aggregating like-to-view ratio: Calculate the ratio based on the final counts of likes and view_count at the end of the trending period\n",
    "        transformed_df['like_view_ratio'] = (transformed_df['likes_end'] / \n",
    "                                            transformed_df['view_count_end']).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        # Popularity score calculation: We use the end values for likes, views, and comments for a final snapshot\n",
    "        transformed_df['popularity_score'] = (transformed_df['comment_count_end'] / \n",
    "                                            transformed_df['view_count_end'] * \n",
    "                                            transformed_df['likes_end']).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        # Calculate percentiles for popularity score\n",
    "        Q1 = transformed_df['popularity_score'].quantile(0.25)\n",
    "        Q3 = transformed_df['popularity_score'].quantile(0.75)\n",
    "\n",
    "        thresholds = {'Q1': Q1, 'Q3': Q3}\n",
    "\n",
    "        # Video popularity classification (adapted from the research paper)\n",
    "        conditions = [\n",
    "            # Class 0: Non-popular videos (views < 100,000)\n",
    "            (transformed_df['view_count_end'] < 100000),\n",
    "            \n",
    "            # Class 1: \"Bad views\" based on popularity score\n",
    "            (transformed_df['popularity_score'] < Q1),\n",
    "            \n",
    "            # Class 3: Videos with overwhelming praise (score > Q3)\n",
    "            (transformed_df['popularity_score'] > Q3)\n",
    "        ]\n",
    "\n",
    "        # Default is Class 2 (neutral videos)\n",
    "        choices = [0, 1, 3]\n",
    "        transformed_df['popularity_class'] = np.select(conditions, choices, default=2)\n",
    "        \n",
    "        logger.info(f\"Transformation complete. DataFrame now has {len(transformed_df.columns)} columns\")\n",
    "        return transformed_df\n",
    "\n",
    "    def preview_dataframe(self, df, rows=5):\n",
    "        \"\"\"\n",
    "        Simple preview of the dataframe\n",
    "        \n",
    "        :param df: DataFrame to preview\n",
    "        :param rows: Number of rows to display\n",
    "        \"\"\"\n",
    "        logger.info(f\"DataFrame preview (first {rows} rows):\")\n",
    "        print(\"\\nDataFrame Preview:\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"\\nSample Data:\")\n",
    "        print(df.head(rows))\n",
    "        print(\"\\nColumn Data Types:\")\n",
    "        print(df.dtypes)\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(df.describe().round(2))\n",
    "\n",
    "    def load_to_gcs(self, df, output_blob_path):\n",
    "        \"\"\"\n",
    "        Load the transformed DataFrame back to GCS as a parquet file\n",
    "        \n",
    "        :param df: Pandas DataFrame to save\n",
    "        :param output_blob_path: Path where to save the parquet file in GCS\n",
    "        :return: GCS URI of the saved file\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading transformed data to gs://{self.bucket_name}/{output_blob_path}\")\n",
    "        \n",
    "        # Create a temporary file to save the parquet\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.parquet')\n",
    "        \n",
    "        df.to_parquet(temp_file.name, index=False)\n",
    "        \n",
    "        # Upload to GCS\n",
    "        blob = self.bucket.blob(output_blob_path)\n",
    "        blob.upload_from_filename(temp_file.name)\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        temp_file.close()\n",
    "        os.unlink(temp_file.name)\n",
    "        \n",
    "        uri = f\"gs://{self.bucket_name}/{output_blob_path}\"\n",
    "        logger.info(f\"Successfully saved transformed data to {uri}\")\n",
    "        return uri\n",
    "\n",
    "    def load_to_bigquery(self, gcs_uri, table_id):\n",
    "        \"\"\"\n",
    "        Load data from GCS to a BigQuery table\n",
    "        \n",
    "        :param gcs_uri: GCS URI of the parquet file\n",
    "        :param table_id: Target BigQuery table ID\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading data from {gcs_uri} to BigQuery table {table_id}\")\n",
    "        \n",
    "        # Define the job configuration for loading parquet\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.PARQUET,\n",
    "            autodetect=True,  # Automatically infer schema from the parquet file\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # Replace existing table\n",
    "        )\n",
    "\n",
    "        # Create the full table reference (project_id.dataset_id.table_id)\n",
    "        table_ref = f\"{self.gcp_project_id}.{self.dataset_id}.{table_id}\"\n",
    "\n",
    "        # Load parquet data into BigQuery\n",
    "        load_job = self.bq_client.load_table_from_uri(\n",
    "            gcs_uri, table_ref, job_config=job_config\n",
    "        )\n",
    "\n",
    "        load_job.result()  # Waits for the job to complete\n",
    "        \n",
    "        # Get the table and print info\n",
    "        table = self.bq_client.get_table(table_ref)\n",
    "        logger.info(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns to {table_ref}\")\n",
    "\n",
    "    def run_pipeline(self, input_blob_path, transformed_blob_path, table_id):\n",
    "        \"\"\"\n",
    "        Execute the complete ETL pipeline\n",
    "        \n",
    "        :param input_blob_path: Path to the input parquet file in GCS\n",
    "        :param transformed_blob_path: Path where to save the transformed parquet file\n",
    "        :param table_id: Target BigQuery table ID\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting ETL pipeline\")\n",
    "        \n",
    "        # Extract\n",
    "        raw_data = self.extract_from_gcs(input_blob_path)\n",
    "        \n",
    "        # Transform\n",
    "        transformed_data = self.transform_data(raw_data)\n",
    "        \n",
    "        # Preview the transformed data before loading\n",
    "        self.preview_dataframe(transformed_data)\n",
    "        \n",
    "        # Load to GCS\n",
    "        transformed_uri = self.load_to_gcs(transformed_data, transformed_blob_path)\n",
    "        \n",
    "        # Load to BigQuery\n",
    "        self.load_to_bigquery(transformed_uri, table_id)\n",
    "        \n",
    "        logger.info(\"ETL pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    GCP_PROJECT_ID = \"aesthetic-nova-454803-r7\"\n",
    "    DATASET_ID = \"youtube_trending_dataset\"\n",
    "    BUCKET_NAME = \"youtube-trending-videos-dataset\"\n",
    "    \n",
    "    # Initialize the ETL pipeline\n",
    "    youtube_etl = YouTubeTrendingETL(GCP_PROJECT_ID, DATASET_ID, BUCKET_NAME)\n",
    "\n",
    "    # Input and output paths\n",
    "    input_blob_path = \"youtube_trending_data/US_youtube_trending_data.parquet\"\n",
    "    transformed_blob_path = \"youtube_trending_data/transformed/US_youtube_trending_data_transformed.parquet\"\n",
    "    table_id = \"US_trending_videos_transformed\"\n",
    "\n",
    "    # Run the pipeline\n",
    "    youtube_etl.run_pipeline(input_blob_path, transformed_blob_path, table_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
